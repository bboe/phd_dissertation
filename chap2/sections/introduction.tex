\section{Introduction}
There is a movement toward both more interactive and more engaging assignments
and languages for introductory and AP computer science courses. This movement
includes the push for Python with Multimedia approaches, the various approaches
to the AP Computer Science Principles course, as well as Alice and
Scratch~\cite{Adams:2012:SLP:2157136.2157319, Forte:2004:CCC:962752.962945,
  Simon:2010:ERC:1822090.1822151, Snyder:2012:FFC:2189835.2189852,
  Cooper:2003:TOI:611892.611966, Maloney:2010:SPL:1868358.1868363}.

One drawback of visual and auditory assignments is that their evaluation can be
more difficult than traditional text-based programming assignments.  A common
and straightforward practice in evaluating text-based assignments is to perform
functional testing. That is, to write a script to run all submitted programs
and compare their output with solution
files~\cite{Jackson:1997:GSP:268084.268210}.  More recently, unit-testing
frameworks have been employed as part of automated
assessment~\cite{Spacco:2006:EMD:1140124.1140131,
  Edwards:2003:RCS:949344.949390}.  When students are given creative freedom
with a sensory assignment---which is an integral feature of languages such as
Alice and Scratch---there is neither a text-based output file to compare to an
expected output, nor a straightforward way to perform unit-testing.  For
example, Scratch evaluation typically requires that each \sprogram{} be
individually opened and run.  Inspection of \sprogram{} code requires many
mouse clicks and navigation through a number of Scratch objects including the
\stage{} and all \emph{sprites} as well as the associated \emph{scripts} of
each.

To assist with assessment of \sprogram{s}, we propose a static analysis tool.
Inspired by the Scratch mascot, a cat, and the concept of lint, a static
analysis utility for C that looks for potential defects with program code, we
call our system Hairball~\cite{Johnson78lint}. We propose two roles for
Hairball:

\begin{description}
\item[formative assessment] Inspired by lint, we envision students will use
  Hairball to check their \sprogram{s} for potential problems.
\item[summative assessment] Researchers and instructors can accelerate manual
  analysis of \sprogram{s} by using Hairball to verify the presence and
  correctness of certain programming constructs, as well as to direct manual
  analysis toward potential problems within these \sprogram{s}.
\end{description}

We developed a plugin architecture so that, in Python, Hairball can be extended
and adapted for evaluation of specific assignments, and tested Hairball on
fifty-eight assignments created by \nth{6}--\nth{8} grade students during our
two-week Scratch-based summer camp in 2012.

The challenges we explore in this chapter relate to where the line should be
drawn between what Hairball can do with static analysis, and where manual
examination of the \sprogram{} is necessary.  We find that each has its own
strengths.  Hairball can quickly differentiate between \sprogram{s} that do, or
do not, contain certain targeted constructs. Hairball is also particularly
helpful for identifying instances of various constructs and implementations
that are not robust but may not immediately cause obvious errors at runtime.
Manual analysis, however, is still needed to evaluate the overall aesthetic
effect and cohesion of a visual or auditory assignment.

We begin in Section~\localref{sec:background} by providing a background on
automated analysis in general and for Scratch in particular. We describe our
Hairball framework in Section~\localref{sec:framework}. The Hairball plugins we
developed for our analysis are described in Section~\localref{sec:plugins}. We
describe our methodology in Section~\localref{sec:methodology}, and results in
Section~\localref{sec:results}. Finally, in Section~\localref{sec:conclude} we
conclude.
