\section{Methodology} \locallabel{sec:methodology}
In the remainder of this study, we will use Hairball to refer to both the
framework and the set of plugins described in Section~\localref{sec:plugins}.

We tested Hairball on the projects submitted during our two-week summer camp.
There were five assignments total, with a distribution of concept
requirements. For example, complex animations were taught toward the end of the
camp, so they were only present in the last two assignments whereas
initialization was present in all~\cite{Franklin:2013:SBO}.

We first performed a manual analysis on all 58 of the submitted Scratch
projects.  Three members of our project staff independently analyzed the first
five projects submitted for a given assignment using a common rubric. We
discussed any discrepancies in our scores, and after coming to agreement, we
analyzed the remaining projects.  Once again, any score discrepancies were
reconciled.

Hairball was then programmed to match the methodology agreed upon by the staff
members when classifying the concepts.  Hairball was run on all of the
projects.  When there were discrepancies between Hairball and the manual
analysis, there was a second manual analysis to determine which was
correct---Hairball or the manual analysis.  In the results section, we compare
the results between Hairball and the reconciled manual analysis using the
second analysis results as a ground truth.

Because the projects are sensory in nature (auditory, visual), we are not
attempting to create Hairball to replace manual analysis.  Instead, we are
automating the identification of the ``easy'' cases in order to accelerate
analysis.  The methodology is not perfect because Hairball was informed by the
manual analysis - sort of like a fourth entity whose answers needed to be
reconciled in the group.  As the results show, Hairball did an excellent job of
identifying issues that all three of our staff members missed.
