\begin{abstract}

\addcontentsline{toc}{chapter}{Abstract}

There is a proliferating demand for newly trained computer scientists as the
number of computer science related jobs continues to increase. Computer science
undergraduate programs have seen a growth in enrollment since 2008, but these
higher numbers of graduating computer scientists are not sufficient to meet the
demand. In fact, university programs will be able to train ample new computer
scientists only when there is more interest and preparation from students in
the primary and secondary educational levels who will eventually move into
university-level courses of study. As a result, significant effort is being
made to incorporate computational thinking into existing primary school
education in order to increase young students' interest in computer
science. However, implementing and validating new curriculum across the nation
is a tremendous challenge for computer science education researchers and
curriculum designers.

To enable wide-scale computer science education we do two things. First, we
create a framework called Hairball to support the static analysis of
\sprogram{s}. Scratch is a popular building-block language utilized to pique
interest in and teach the basics of computer science. Hairball allows for rapid
curriculum alterations and thus contributes to wide-scale deployment of
\nth{4}--\nth{6} grade computer science curriculum.  We describe how we utilize
Hairball to ensure that learning objectives are met in our pilot computational
thinking curriculum targeted for fourth, fifth, and sixth grade students.

Second, we create a real-time feedback and assessment system utilized
extensively by students in seven instances of two University of California,
Santa Barbara computer science courses from Winter Quarter 2013 through Spring
Quarter 2014. The configuration of the system was systematically varied from
assignment to assignment and between instances of each course to measure
changes in student submission behavior. We show that changes to the system
configuration affect student submission behavior. For instance, increasing the
time between when students make a submission and when they receive feedback
correlates with an increase in score improvement between submissions. The
insights from our analysis of student submission behavior have a positive
impact on supporting not only the way students learn and progress through
course material, but also on instructor behavior making it possible for
instructors to tailor assignments and associated assignment parameters to
optimize student learning, and reduce the amount of time they devote to student
assignment assessment.

\end{abstract}
