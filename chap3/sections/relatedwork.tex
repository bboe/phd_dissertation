\section{Related Work}\locallabel{sec:related}
Significant prior work has focused on teaching young students to program in the
context of outreach programs such as after school programs and summer camps
where students learn to program using languages such as \emph{Alice} or
\emph{Scratch}. \emph{Computer Science Unplugged} is another approach to help
students learn computer science concepts without the use of a computer. With
these curricula have come a number of improvements in how to assess these young
studentsâ€™ programming capabilities.

While early work focused on student surveys to assess student attitudes
regarding the concepts taught in these programs, more recent work performed
detailed assessment of the final assignments that students produced in order to
see what students learned based on the computer science concepts present in the
work. Maloney et al.\ analyzed 536 completed \sprogram{s} created by young
students over an 18-month period in an after school program. They found that
these students demonstrated an ability to use key programming concepts with
help only from inexperienced
mentors~\cite{Maloney:2008:PCU:1352135.1352260}. Wilson et al.\ adapted the
coding scheme of Denner et al.\ to identify the most frequently used
programming concepts by children who created games in
Scratch~\cite{Denner:2012:CGC:2072695.2073050, wilson12}. Using their Progress
of Early Computational Thinking Model on an existing set of \sprogram{s},
Seiter and Foreman attempt to identify differences in computational thinking
comprehension between students in \nth{1}--\nth{6}
grade~\cite{Seiter:2013:MLP:2493394.2493403}. While their work provides an
excellent example of extracting student understanding from completed
\sprogram{s}, their results depend only on the presence of a computer science
concept in a \sprogram{} to determine if a student understood the
concept. Brennan and Resnick report that the presence of a concept in a
student's \sprogram{} is not indicative of the student's understanding of the
concept, especially when modifying existing \sprogram{s} is strongly
encouraged~\cite{brennan12}.

Through a combination of field notes collected during observation of student
work along with both manual and automated analysis of these students'
\sprogram{s}, Franklin et al.\ attempted to more precisely determine what
computer science concepts these middle school students had learned during a
two-week summer camp. Furthermore, Franklin et al.\ measured their students'
ability to apply taught concepts to the camp's final
assignment~\cite{Boe:2013:HLS:2445196.2445265, Franklin:2013:SBO}. However, as
reported by Piech et al.\, student understanding of computer science concepts
is not entirely reflected by the student's final version of an assignment. By
using machine learning on the sequence of student in-progress programs, i.e,
snapshots, Piech et al\. reported a correlation between success in the class
and the method through which students solved an
assignment~\cite{Piech:2012:MSL:2157136.2157182}. This correlation was shown to
be more significant than the final score on an assignment, suggesting that
future analysis should look at multiple snapshots in order to more accurately
evaluate student understanding.

Our work uses a combination of field notes, and both manual and automated
assessment of student's in-progress work in order to provide a depth of
knowledge about student's understanding of our curriculum's concepts.
