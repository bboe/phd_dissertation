\section{Related Work}\locallabel{sec:related}
Significant work has focused on teaching young students the basics of computer
science in the context of outreach programs such as summer camps and after
school programs. In these programs, young students learn the basics of computer
science using languages such as \emph{Alice} or \emph{Scratch}. \emph{Computer
  Science Unplugged} is another approach to help students learn computer
science concepts without the use of a computer. Along with the curricula for
these outreach programs, researchers have also developed improved ways of
evaluating student success with computer programming in these languages.

Early research pertaining to assessment focused on student surveys to assess
student attitudes about the concepts they were taught in these outreach
programs. More recent work, however, has moved toward detailed assessment of
the computer science concepts applied in students' completed assignments to
discover how these assignments reflect what the students have learned. An
example of this type of study was completed by Maloney et al.\, where they
analyzed 536 completed \sprogram{s} created by young students over an 18-month
period in an after school program. They found that students demonstrated an
ability to use key programming concepts with help only from inexperienced
mentors~\cite{Maloney:2008:PCU:1352135.1352260}. Wilson et al.\ adapted the
coding scheme of Denner et al.\ to identify the most frequently used
programming concepts by children who created games in
Scratch~\cite{Denner:2012:CGC:2072695.2073050, wilson12}. Using their Progress
of Early Computational Thinking Model on an existing set of \sprogram{s},
Seiter and Foreman worked to identify differences in computational thinking
comprehension between students in \nth{1}--\nth{6}
grade~\cite{Seiter:2013:MLP:2493394.2493403}. While their work provides an
excellent example of extracting student understanding from completed
\sprogram{s}, their results depend on the inclusion of a computer science
concept in a \sprogram{} to determine whether a student appears to understand
the concept. Brennan and Resnick argue, however, that the mere inclusion of a
concept in a student's \sprogram{} is not indicative of the student's
understanding of the concept, especially when encouraged to modify existing
\sprogram{s}~\cite{brennan12}.

Through a combination of field notes collected during observation of student
work along with both manual and automated analysis of these students'
\sprogram{s}, Franklin et al.\ attempted to more precisely determine what
computer science concepts these middle school students had learned during a
two-week summer camp. Furthermore, Franklin et al.\ measured their students'
ability to apply taught concepts to the camp's final
assignment~\cite{Boe:2013:HLS:2445196.2445265, Franklin:2013:SBO}. However, as
reported by Piech et al.\, student understanding of computer science concepts
is not entirely reflected by the student's final version of an assignment. By
using machine learning on the sequence of student in-progress programs, i.e,
snapshots, Piech et al\. reported a correlation between success in the class
and the method through which students solved an
assignment~\cite{Piech:2012:MSL:2157136.2157182}. This correlation was shown to
be more significant than the final score on an assignment, suggesting that
future analysis should look at multiple snapshots in order to more accurately
evaluate student understanding.

Our work uses a combination of field notes along with both manual and automated
assessment of student's in-progress work to provide a depth of knowledge about
student understanding of our curriculum concepts.
