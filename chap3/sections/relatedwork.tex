\section{Related Work}\locallabel{sec:related}
The development and refinement of outreach programs using curricula based on CS
Unplugged, Scratch, and Alice have matured in their goals and assessments
through time. Early assessments used surveys to gauge student attitudes, and
later assessments inspected final projects or tests to determine what students
knew at the end. These have provided a glimpse of what students are capable of
at different ages. The next step is to create a picture of \emph{how} students
learn in order to inform future curricula.

Researchers have mined the wealth of completed Scratch projects to determine
what concepts were displayed in completed projects. Over 500 Scratch projects
created by urban youth during an afterschool program showed that youth used key
programming concepts with no specific instructional interventions and only
inexperienced mentors~\cite{Maloney:2008:PCU:1352135.1352260}. Wilson et
al. adapted Denner et al.'s prior coding scheme to identify the most common
programming concepts used by children who created games with
Scratch~\cite{Denner:2012:CGC:2072695.2073050, wilson12}. Another effort used
existing projects across different grade levels to identify how computational
thinking concepts varied by level in their Progression of Early Computational
Thinking (PECT) Model~\cite{Seiter:2013:MLP:2493394.2493403}. This work is an
excellent example of extracting information from existing Scratch
projects. Unfortunately, it shares the limitation with earlier work of assuming
that if students use a concept in a project, they have learned it. Brennan et
al. showed that using a concept in a project does not necessarily mean that
students understand the concept, especially when ``re-mixing,'' or modifying
existing projects, is strongly encouraged~\cite{brennan12}.

Several more recent studies have sought a deeper understanding, using a blend
of methods. Franklin et al. used a combination of field notes, hand analysis,
and automated analysis of Scratch projects to explore which concepts middle
school students learned during a 2-week summer camp and which of these they
were able to transfer to a culminating
project~\cite{Boe:2013:HLS:2445196.2445265, Franklin:2013:SBO}. Piech et
al. added to the knowledge that assessing only the final project does not
reflect actual student understanding. They used machine learning on sets of
snapshots, showing that the path students take to completing the final project
had a much stronger correlation to future success than the final state of the
project~\cite{Piech:2012:MSL:2157136.2157182}. They did not, however, inspect
the projects to determine what those different paths represented. As part of
the effort to understand variation in computational thinking across students,
Werner et al. developed a performance assessment tool to assess algorithmic
thinking and effective use of abstraction and modeling among middle school
students~\cite{Werner:2012:FPA:2157136.2157200}.

Our work is distinct because of the blend of qualitative and quantitative
research methods and the depth of knowledge it presents. The qualitative data
and analysis provide depth of understanding as to how students learn and their
partial understandings, whereas the quantative analysis provides insight as to
how many students possess particular partial understandings.
