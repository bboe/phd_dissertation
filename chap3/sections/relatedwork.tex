\section{Related Work}\locallabel{sec:related}
A significant amount of prior work has looked at how to teach young students to
program. Much of this work focuses on after school programs and summer camps
where students learn to program using languages such as \emph{Alice} or
\emph{Scratch}. \emph{Computer Science Unplugged} is another approach to help
students learn computer science concepts without the use of a computer. With
these curricula have come a number of improvements in how to assess these young
studentsâ€™ programming capabilities.

While early work focused on student surveys to assess student attitudes
regarding the concepts taught in these programs, more recent work performed
detailed assessment of the final assignments that students produced in order to
see what students learned based on the computer science concepts present in the
work. Maloney et al.\ analyzed 536 completed \sprogram{s} created by young
students over an 18-month period in an after school program. They found that
these students demonstrated an ability to use key programming concepts with
help only from inexperienced
mentors~\cite{Maloney:2008:PCU:1352135.1352260}. Wilson et al.\ adapted the
coding scheme of Denner et al.\ to identify the most frequently used
programming concepts by children who created games in
Scratch~\cite{Denner:2012:CGC:2072695.2073050, wilson12}. Using their Progress
of Early Computational Thinking Model on an existing set of \sprogram{s},
Seiter and Foreman attempt to identify differences in computational thinking
comprehension between students in \nth{1}--\nth{6}
grade~\cite{Seiter:2013:MLP:2493394.2493403}. While their work provides an
excellent example of extracting student understanding from completed
\sprogram{s}, their results depend only on the presence of a computer science
concept in a \sprogram{} to determine if a student understood the
concept. Brennan and Resnick report that the presence of a concept in a
student's \sprogram{} is not indicative of the student's understanding of the
concept, especially when modifying existing \sprogram{s} is strongly
encouraged~\cite{brennan12}.

Several more recent studies have sought a deeper understanding, using a blend
of methods. Franklin et al.\ used a combination of field notes, manual
analysis, and automated analysis of \sprogram{s} to explore which concepts
middle school students learned during a two-week summer camp, and which of
these concepts they were able to transfer to a culminating
assignment~\cite{Boe:2013:HLS:2445196.2445265, Franklin:2013:SBO}. Piech et
al.\ reported that assessment of only the final version of an assignment does
not reflect actual student understanding. They used machine learning on sets of
snapshots, showing that the path students take to complete an assignment had a
much stronger correlation to future success than the final score of an
assignment~\cite{Piech:2012:MSL:2157136.2157182}. They did not, however,
inspect the snapshots to determine what those different paths represented. As
part of the effort to understand variation in computational thinking across
students, Werner et al.\ developed a performance assessment tool to assess
algorithmic thinking and effective use of abstraction and modeling among middle
school students~\cite{Werner:2012:FPA:2157136.2157200}.

Our work is distinct because of the blend of qualitative and quantitative
research methods and the depth of knowledge it presents. The qualitative data
and analysis provide depth of understanding as to how students learn and their
partial understandings; whereas the quantitative analysis provides insight as
to how many students possess particular partial understandings.
