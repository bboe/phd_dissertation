\section{Related Work}\locallabel{sec:related}
The development and refinement of outreach programs using curricula based on
\emph{CS Unplugged}, \emph{Scratch}, and \emph{Alice} have matured in their
goals and assessments through time. Early assessments used surveys to gauge
student attitudes, and later assessments inspected final assignments and tests
to determine what students knew at the end of the curricula. These assessments
have provided a glimpse of what students are capable of at different ages. The
next step is to create a picture of \emph{how} students learn in order to
inform future curricula.

Researchers have mined the wealth of completed \sprogram{s} to determine what
concepts were displayed in these \sprogram{s}. Over 500 \sprogram{s} created by
young students during an afterschool program showed that these students used
key programming concepts with no specific instructional interventions and only
inexperienced mentors~\cite{Maloney:2008:PCU:1352135.1352260}. Wilson et
al.\ adapted Denner et al.'s prior coding scheme to identify the most common
programming concepts used by children who created games with
Scratch~\cite{Denner:2012:CGC:2072695.2073050, wilson12}. Another effort used
existing \sprogram{s} across different grade levels to identify how
computational thinking concepts varied by level in their Progression of Early
Computational Thinking Model~\cite{Seiter:2013:MLP:2493394.2493403}. This work
is an excellent example of extracting information from existing
\sprogram{s}. Unfortunately, it shares the limitation with earlier work of
assuming that if students use a concept in a \sprogram{}, they have learned
it. Brennan et al.\ showed that using a concept in a \sprogram{} does not
necessarily mean that students understand the concept, especially when
\emph{re-mixing}, i.e., modifying an existing \sprogram{}, is strongly
encouraged~\cite{brennan12}.

Several more recent studies have sought a deeper understanding, using a blend
of methods. Franklin et al.\ used a combination of field notes, manual
analysis, and automated analysis of \sprogram{s} to explore which concepts
middle school students learned during a two-week summer camp, and which of
these concepts they were able to transfer to a culminating
assignment~\cite{Boe:2013:HLS:2445196.2445265, Franklin:2013:SBO}. Piech et
al.\ added to the knowledge that assessing only the final assignment does not
reflect actual student understanding. They used machine learning on sets of
snapshots, showing that the path students take to complete an assignment had a
much stronger correlation to future success than the final score of an
assignment~\cite{Piech:2012:MSL:2157136.2157182}. They did not, however,
inspect the snapshots to determine what those different paths represented. As
part of the effort to understand variation in computational thinking across
students, Werner et al.\ developed a performance assessment tool to assess
algorithmic thinking and effective use of abstraction and modeling among middle
school students~\cite{Werner:2012:FPA:2157136.2157200}.

Our work is distinct because of the blend of qualitative and quantitative
research methods and the depth of knowledge it presents. The qualitative data
and analysis provide depth of understanding as to how students learn and their
partial understandings; whereas the quantitative analysis provides insight as
to how many students possess particular partial understandings.
